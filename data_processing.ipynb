{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd \n","import os \n","from multiprocessing import Pool\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#read the dataset in chunks\n","def process_chunk(file_path, chunk_size = 100000):\n","    chunks = pd.read_csv(file_path,chunksize=chunk_size)\n","    processed_data = []\n","\n","    #perform data transformation(eg: mean income etc ..)\n","    for chunk in chunks:\n","        chunk['income_per_capita'] = chunk['PINCP']/chunk['NP']\n","        processed_data.append(chunk)\n","    \n","    #Concatenate all processed chunks\n","    processed_df = pd.concat(processed_data)\n","\n","    #save the processed data \n","    output_file_path = f\"processed_{os.path.basename(file_path)}\"\n","    processed_df.to_csv(output_file_path, index=False)\n","    return output_file_path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def main():\n","    input_dir = \"path/to/your/dataset\"\n","    files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.csv')]\n","    \n","    # Use multiprocessing to process files in parallel\n","    with Pool(processes=4) as pool:\n","        results = pool.map(process_chunk, files)\n","    \n","    print(\"Processed files:\", results)\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
